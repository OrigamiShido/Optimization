{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T14:58:59.179985Z",
     "start_time": "2024-05-03T14:57:53.603476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14841318\n",
      "[[-2.0623989]\n",
      " [ 0.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]]\n",
      "[[21.99999988 -6.19272467  6.19271967 51.11162465]\n",
      " [-6.34020697  0.          0.          0.        ]\n",
      " [ 6.28805064  0.          0.          0.        ]\n",
      " [46.31880518  0.          0.          0.        ]]\n",
      "最速下降法:\n",
      "迭代次数:820\n",
      "运行时间:1.0485796928405762s\n",
      "最优解为:\n",
      "[[0.19385489]\n",
      " [0.16561179]\n",
      " [0.11663432]\n",
      " [0.12450401]]\n",
      "最小值:0.00030886\n",
      "阻尼牛顿法:\n",
      "迭代次数:122\n",
      "运行时间:0.4381828308105469s\n",
      "最优解为:\n",
      "[[0.19143365]\n",
      " [0.22210067]\n",
      " [0.12931595]\n",
      " [0.14991261]]\n",
      "最小值:0.00030907\n",
      "牛顿法:\n",
      "迭代次数:53\n",
      "运行时间:0.07496261596679688s\n",
      "最优解为:\n",
      "[[-1.05047168e+00]\n",
      " [-6.12759067e+02]\n",
      " [ 6.14638827e+02]\n",
      " [ 6.84509712e+03]]\n",
      "最小值:0.03737007\n",
      "拟牛顿法:\n",
      "迭代次数:4846\n",
      "运行时间:5.650827407836914s\n",
      "最优解为:\n",
      "[[0.19297973]\n",
      " [0.18705423]\n",
      " [0.12170702]\n",
      " [0.13431   ]]\n",
      "最小值:0.00030754\n",
      "FR共轭梯度法:\n",
      "迭代次数:100000\n",
      "运行时间:56.86339712142944s\n",
      "最优解为:\n",
      "[[0.19301836]\n",
      " [0.0030953 ]\n",
      " [0.00556666]\n",
      " [0.05702301]]\n",
      "最小值:0.00073382\n",
      "LMF法:\n",
      "迭代次数:444\n",
      "运行时间:0.3400115966796875s\n",
      "最优解为:\n",
      "[[0.19385556]\n",
      " [0.16570258]\n",
      " [0.11668666]\n",
      " [0.12454144]]\n",
      "最小值:0.00030885\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from sympy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def Speedest(f,x,epsilon):\n",
    "    '''\n",
    "    最速下降法\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: epsilon\n",
    "    :return: [迭代次数，计算时间，最优解，最小值]\n",
    "    '''\n",
    "    start=time.time()\n",
    "    cnt = 0  # 计数标志\n",
    "    xk = x\n",
    "    gk = grad(f, xk)\n",
    "    while (np.linalg.norm(gk) > epsilon):\n",
    "        dk = -grad(f, xk)\n",
    "        alpha = Armijo(f, xk, dk, 0.5)  # Armijo求步长\n",
    "        xk = xk + alpha * dk  # 迭代\n",
    "        gk = grad(f, xk)  # 迭代后点处的梯度\n",
    "        cnt = cnt + 1  # 迭代次数+1\n",
    "        if(cnt==100000):break\n",
    "    end = time.time()\n",
    "    print('最速下降法:')\n",
    "    print('迭代次数:%d'% cnt)\n",
    "    print('运行时间:'+(str)(end-start)+'s')\n",
    "    print('最优解为:\\n'+(str)(xk))\n",
    "    print('最小值:%.8f'% f(xk))\n",
    "    return True\n",
    "\n",
    "def Newton(f,x,epsilon):\n",
    "    '''\n",
    "    牛顿法\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: epsilon\n",
    "    :return: [迭代次数，计算时间，最优解，最小值]\n",
    "    '''\n",
    "    start = time.time()\n",
    "    cnt = 0  # 计数标志\n",
    "    xk = x\n",
    "    gk = grad(f, xk)  # 梯度\n",
    "    while (np.linalg.norm(gk) > epsilon):  # 迭代条件:梯度的范数大于epsilon\n",
    "        if (np.linalg.det(Hessian(f, xk)) < 0.00001):\n",
    "            dk = -gk\n",
    "        else:\n",
    "            dk = -(np.linalg.inv(Hessian(f, xk)).dot(gk))  # 求方向\n",
    "        xk = xk + dk  # 迭代\n",
    "        gk = grad(f, xk)  # 迭代后点处的梯度\n",
    "        cnt = cnt + 1  # 迭代次数+1\n",
    "        if (cnt == 100000): break\n",
    "    end = time.time()\n",
    "    print('牛顿法:')\n",
    "    print('迭代次数:%d' % cnt)\n",
    "    print('运行时间:' + (str)(end - start) + 's')\n",
    "    print('最优解为:\\n' + (str)(xk))\n",
    "    print('最小值:%.8f' % f(xk))\n",
    "    return True\n",
    "\n",
    "\n",
    "def DampingNewton(f,x,epsilon):\n",
    "    '''\n",
    "        阻尼牛顿法\n",
    "        :param f: 函数\n",
    "        :param x: 初始点\n",
    "        :param epsilon: epsilon\n",
    "        :return: [迭代次数，计算时间，最优解，最小值]\n",
    "        '''\n",
    "    start=time.time()\n",
    "    cnt=0       #计数标志\n",
    "    xk=x\n",
    "    gk=grad(f,xk)       #梯度\n",
    "    while(np.linalg.norm(gk)>epsilon): #迭代条件:梯度的范数大于epsilon\n",
    "        if(np.linalg.det(Hessian(f,xk))<0.0001):\n",
    "            dk=-gk\n",
    "        else:\n",
    "            dk=-(np.linalg.inv(Hessian(f,xk)).dot(gk))    #求方向\n",
    "        #alpha=Armijo(f,xk,dk,0.5)                       #求步长\n",
    "        alpha=StepSize(f,xk,dk)\n",
    "        xk=xk+alpha*dk                                  #迭代\n",
    "        gk=grad(f,xk)                               #迭代后点处的梯度\n",
    "        cnt=cnt+1                                       #迭代次数+1\n",
    "        if(cnt==100000): break\n",
    "    end=time.time()\n",
    "    print('阻尼牛顿法:')\n",
    "    print('迭代次数:%d' % cnt)\n",
    "    print('运行时间:' + (str)(end - start) + 's')\n",
    "    print('最优解为:\\n' + (str)(xk))\n",
    "    print('最小值:%.8f' % f(xk))\n",
    "    return True\n",
    "\n",
    "def DFP(f,xk,epsilon):\n",
    "    '''\n",
    "    DFP法\n",
    "    :param f:函数\n",
    "    :param xk:初始点\n",
    "    :param epsilon:epsilon\n",
    "    :return: [迭代次数，计算时间，最优解，最小值]\n",
    "    '''\n",
    "    start=time.time()\n",
    "    cnt=0\n",
    "    n=np.size(xk)\n",
    "    H=np.mat(np.eye(n))\n",
    "    while(mo(grad(f,xk))>epsilon):\n",
    "        cnt+=1\n",
    "        dk=-H.dot(grad(f,xk))\n",
    "        alpha=Armijo(f,xk,dk,0.5)\n",
    "        sk=alpha*dk\n",
    "        yk=grad(f,xk+alpha*dk)-grad(f,xk)\n",
    "        if(mo(sk.T.dot(yk))==0 or mo(yk.T.dot(H))==0):\n",
    "            H=np.eye(n)\n",
    "        else:\n",
    "            H=H+(sk.dot(sk.T))/mo(sk.T.dot(yk))-(H.dot(yk).dot(yk.T).dot(H))/mo(yk.T.dot(H).dot(yk))\n",
    "        xk = xk + alpha * dk\n",
    "        if (cnt == 100000): break\n",
    "    end=time.time()\n",
    "    print('拟牛顿法:')\n",
    "    print('迭代次数:%d' % cnt)\n",
    "    print('运行时间:' + (str)(end - start) + 's')\n",
    "    print('最优解为:\\n' + (str)(xk))\n",
    "    print('最小值:%.8f' % f(xk))\n",
    "    return True\n",
    "\n",
    "def FR(f,x,epsilon):\n",
    "    '''\n",
    "    FR共轭梯度法\n",
    "    :param f: 函数\n",
    "    :param x: 初始迭代点x0\n",
    "    :param epsilon: epsilon\n",
    "    :return: [迭代次数，计算时间，最优解，最小值]\n",
    "    '''\n",
    "    start=time.time()\n",
    "    cnt=0 #计数标志\n",
    "    xk=x\n",
    "    g_pre=grad(f,xk)   #xk-1点的梯度,第一次迭代时，梯度任意\n",
    "    g_now=grad(f,xk)   #xk点的梯度\n",
    "    while(mo(g_now)>epsilon):\n",
    "        cnt+=1\n",
    "        if(cnt==1):\n",
    "            dk=-g_now\n",
    "        else:\n",
    "            beta = ((g_now.T.dot(g_now)) / (g_pre.T.dot(g_pre)))[0,0]\n",
    "            dk=-g_now+beta*g_pre\n",
    "        alpha=Armijo(f,xk,dk,0.5)\n",
    "        xk=xk+alpha*dk          #计算xk+1\n",
    "        g_pre=g_now       #对gk-1进行迭代\n",
    "        g_now=grad(f,xk)     #对gk进行迭代\n",
    "        if(cnt==100000):break\n",
    "    end=time.time()\n",
    "    print('FR共轭梯度法:')\n",
    "    print('迭代次数:%d' % cnt)\n",
    "    print('运行时间:' + (str)(end - start) + 's')\n",
    "    print('最优解为:\\n' + (str)(xk))\n",
    "    print('最小值:%.8f' % f(xk))\n",
    "    return True\n",
    "\n",
    "def LMF(f,x,epsilon):\n",
    "    '''\n",
    "    LMF方法\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: eosilon\n",
    "    :return: [迭代次数，计算时间，最优解，最小值]\n",
    "    '''\n",
    "    start=time.time()\n",
    "    cnt=0                           #计数标志\n",
    "    xk=x\n",
    "    vk=1\n",
    "    gk=grad(f,xk)\n",
    "    while(mo(gk)>epsilon):\n",
    "        cnt += 1\n",
    "        J = (np.mat)(np.zeros([11, 4]))\n",
    "        rk= (np.mat)(np.zeros([11,1]))\n",
    "        for i in range(0,11):                   #计算Jaccobi行列式\n",
    "            r=lambda x:y[i,0]-f_bar(x,t[i,0])       #定义ri(x)\n",
    "            gr=grad(r,xk)\n",
    "            for j in range(0,4):\n",
    "                J[i,j]=gr[j,0]\n",
    "            rk[i,0]=r(xk)\n",
    "        I=(np.mat)(np.eye(4))\n",
    "        J = (np.float64)(J)\n",
    "        I = (np.float64)(I)\n",
    "        rk= (np.float64)(rk)\n",
    "        if(abs(np.linalg.det(J.T.dot(J)+vk*I))<0.1):\n",
    "            dk=-gk\n",
    "        else:\n",
    "            dk=np.linalg.inv(J.T.dot(J)+vk*I).dot(-J.T.dot(rk))\n",
    "        f_delta=f(xk)-f(xk+dk)\n",
    "        q_delta=0.5*(dk.T.dot(vk*dk-gk))[0,0]\n",
    "        gama=f_delta/q_delta\n",
    "        if(gama<0.25):\n",
    "            vk=4*vk\n",
    "        elif (gama>0.75):\n",
    "            vk=0.5*vk\n",
    "        if(gama>0):\n",
    "            xk=xk+dk\n",
    "            gk=grad(f,xk)\n",
    "        if(cnt==100000):break\n",
    "    end=time.time()\n",
    "    print('LMF法:')\n",
    "    print('迭代次数:%d' % cnt)\n",
    "    print('运行时间:' + (str)(end - start) + 's')\n",
    "    print('最优解为:\\n' + (str)(xk))\n",
    "    print('最小值:%.8f' % f(xk))\n",
    "    return True\n",
    "\n",
    "def mo(xk):      #求模\n",
    "    return np.linalg.norm(xk)\n",
    "\n",
    "def grad(f, x):\n",
    "    '''\n",
    "    求梯度\n",
    "    :param f: 函数\n",
    "    :param x: 向量\n",
    "    :return: 梯度向量\n",
    "    '''\n",
    "    delta=0.0000001\n",
    "    gradmatrix=np.zeros(x.shape)\n",
    "    fx=f(x)\n",
    "    it=np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix=it.multi_index\n",
    "        old_value=(np.float64)(x[ix])\n",
    "        x[ix]=(np.float64)(old_value+delta)\n",
    "        fxd=f(x)\n",
    "        gradmatrix[ix]=(fxd-fx)/delta\n",
    "        x[ix]=old_value\n",
    "        it.iternext()\n",
    "    return gradmatrix\n",
    "\n",
    "def Hessian(f,x):\n",
    "    '''\n",
    "    求Hessian矩阵\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: epsilon\n",
    "    :return: f在x处的Hessian矩阵\n",
    "    '''\n",
    "    delta=0.001\n",
    "    n=np.size(x)\n",
    "    HessianMatrix=np.zeros((n,n))\n",
    "    gx0=grad(f,x)\n",
    "    for i in range(0,n):\n",
    "        old_value = (np.float64)(x[i,0])\n",
    "        x[i,0] = (np.float64)(old_value + delta)\n",
    "        gxk=grad(f,x)\n",
    "        for j in range(0,n):\n",
    "            HessianMatrix[i,j]=(np.float64)((gxk[j,0]-gx0[j,0])/delta)\n",
    "        x[i,0]=old_value\n",
    "    return HessianMatrix\n",
    "\n",
    "def Armijo(f,xk,dk,rho):\n",
    "    '''\n",
    "    用Armijo算法求步长\n",
    "    f:函数\n",
    "    xk:函数原始点\n",
    "    dk：原始方向向量\n",
    "    rho:rho\n",
    "    '''\n",
    "    cnt=0\n",
    "    alpha=1                             #初始化alpha\n",
    "    vtr=xk\n",
    "    vtr_alpha=xk+alpha*dk\n",
    "    #while(f(vtr_alpha)>f(vtr)+rho*alpha*grad(f,xk).T.dot(dk) or f(vtr_alpha)<f(vtr)+(1-rho)*alpha*grad(f,xk).T.dot(dk)): #所需满足的条件\n",
    "    while (f(vtr_alpha) > f(vtr) + rho * alpha * grad(f, xk).T.dot(dk) ):  # 所需满足的条件\n",
    "        alpha=rho*alpha\n",
    "        vtr_alpha = xk + alpha * dk\n",
    "    return alpha\n",
    "\n",
    "def StepSize(f,xk,dk):\n",
    "    '''\n",
    "    二元函数一维精确线搜索求步长\n",
    "    :param f: 二元函数\n",
    "    :param xk: 函数点\n",
    "    :param dk: 方向\n",
    "    :return: 步长值\n",
    "    '''\n",
    "    if(dot(dot(dk.T,Hessian(f,xk)),dk)[0,0]==0):  #分母为0的情况给定步长为0.01\n",
    "        return 0.01\n",
    "    else:\n",
    "        alpha=-(grad(f,xk).T.dot(dk))/(dot(dot(dk.T,Hessian(f,xk)),dk))\n",
    "        return alpha[0,0]\n",
    "\n",
    "def f(x):\n",
    "    x1=x[0,0]\n",
    "    x2=x[1,0]\n",
    "    x3=x[2,0]\n",
    "    x4=x[3,0]\n",
    "    result=0\n",
    "    for i in range(0,11):\n",
    "        result+=(y[i,0]-(x1*(t[i,0]**2+x2*t[i,0])/(t[i,0]**2+x3*t[i,0]+x4)))**2\n",
    "    return result\n",
    "\n",
    "def f_bar(x,t):\n",
    "    x1 = x[0, 0]\n",
    "    x2 = x[1, 0]\n",
    "    x3 = x[2, 0]\n",
    "    x4 = x[3, 0]\n",
    "    result=x1*(t**2+x2*t)/(t**2+x3*t+x4)\n",
    "    return result\n",
    "\n",
    "def g(x):\n",
    "    x1 = x[0, 0]\n",
    "    x2 = x[1, 0]\n",
    "    x3 = x[2, 0]\n",
    "    x4 = x[3, 0]\n",
    "    return 3*x1**3+2*x2**2+3*x3+x4*x3**2\n",
    "\n",
    "y = np.mat([[0.1957], [0.1947], [0.1735], [0.1600], [0.0844], [0.0627],\n",
    "            [0.0456], [0.0342], [0.0323], [0.0235], [0.0246]])\n",
    "t = np.mat([[4.0000], [2.0000], [1.0000], [0.5000], [0.2500], [0.1670],\n",
    "            [0.1250], [0.1000], [0.0833], [0.0714], [0.0625]])\n",
    "xk=np.mat([[0.0],[0.0],[0.0],[0.0]])\n",
    "print(f(xk))\n",
    "print(grad(f,xk))\n",
    "print(Hessian(f,xk))\n",
    "Speedest(f,xk,0.0001)\n",
    "DampingNewton(f,xk,0.0001)\n",
    "Newton(f,xk,0.0001)\n",
    "DFP(f,xk,0.0001)\n",
    "FR(f,xk,0.0001)\n",
    "LMF(f,xk,0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
