{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e802045838ea7f3a",
   "metadata": {},
   "source": [
    "# 应用外点罚函数方法和拉格朗日乘子法求解约束最优化问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T07:51:56.189721Z",
     "start_time": "2024-05-15T07:51:55.999453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt= 1\n",
      "cnt= 2\n",
      "最优解为： [[1.10492417 1.19674089 1.53523167]]\n",
      "最优值为： [[0.03256705]]\n",
      "迭代次数为： 2\n",
      "运行时间为： 0.1712644100189209\n",
      "理想最优解： [[0.0325682]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "def targetfunction(x):# x是列向量\n",
    "    return (x[0]-1)**2+(x[0]-x[1])**2+(x[1]-x[2])**4\n",
    "\n",
    "def limitation(x):# 约束条件，x是列向量\n",
    "    if -10 <= x[0] <= 10 and -10 <= x[1] <= 10 and -10 <= x[2] <= 10:\n",
    "        if x[0]*(1+x[1]**2)+x[2]**4-4-3*math.sqrt(2)==0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def equalization(x):# 等式约束条件，x是列向量\n",
    "    return (x[0]*(1+x[1]**2)+x[2]**4-4-3*math.sqrt(2.0))**2\n",
    "\n",
    "def inequalization(x):# 不等式约束条件，x是列向量\n",
    "    return (min(x[0]+10,0))**2+(min(10-x[0],0))**2+(min(x[1]+10,0))**2+(min(10-x[1],0))**2+(min(x[2]+10,0))**2+(min(10-x[2],0))**2\n",
    "\n",
    "def penaltyfunction(x,miu):# 外点罚函数\n",
    "    return targetfunction(x)+0.5*miu*(equalization(x)+inequalization(x))\n",
    "\n",
    "def grad(f, x):\n",
    "    '''\n",
    "    求梯度\n",
    "    :param f: 函数\n",
    "    :param x: 向量\n",
    "    :return: 梯度向量\n",
    "    '''\n",
    "    delta=0.0000001\n",
    "    gradmatrix=np.zeros(x.shape)\n",
    "    fx=f(x)\n",
    "    it=np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix=it.multi_index\n",
    "        old_value=(np.float64)(x[ix])\n",
    "        x[ix]=(np.float64)(old_value+delta)\n",
    "        fxd=f(x)\n",
    "        gradmatrix[ix]=(fxd-fx)/delta\n",
    "        x[ix]=old_value\n",
    "        it.iternext()\n",
    "    return np.mat(gradmatrix)\n",
    "\n",
    "def Hessian(f,x):\n",
    "    '''\n",
    "    求Hessian矩阵\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: epsilon\n",
    "    :return: f在x处的Hessian矩阵\n",
    "    '''\n",
    "    delta=1e-3\n",
    "    n=np.size(x)\n",
    "    HessianMatrix=np.zeros((n,n))\n",
    "    gx0=grad(f,x)\n",
    "    for i in range(0,n):\n",
    "        old_value = (np.float64)(x[i,0])\n",
    "        x[i,0] = (np.float64)(old_value + delta)\n",
    "        gxk=grad(f,x)\n",
    "        for j in range(0,n):\n",
    "            HessianMatrix[i,j]=(np.float64)((gxk[j,0]-gx0[j,0])/delta)\n",
    "        x[i,0]=old_value\n",
    "    return np.mat(HessianMatrix)\n",
    "\n",
    "def Armijo(f,xk,dk,rho):# 非精确线搜索，采用armijo方法\n",
    "    alpha=1\n",
    "    beta=rho\n",
    "    sigma=0.2\n",
    "    while True:\n",
    "        if f(xk+alpha*dk)<=f(xk)+sigma*alpha*grad(f,xk).T*dk:\n",
    "            break\n",
    "        alpha*=beta\n",
    "    return alpha\n",
    "\n",
    "def dampnewton(function,x,epsilon):# 阻尼牛顿法\n",
    "    xk=x\n",
    "    gk=grad(function,xk)\n",
    "    while(np.linalg.norm(gk)>epsilon):\n",
    "        if (np.linalg.det(Hessian(function,xk))<epsilon):\n",
    "            dk=-gk\n",
    "        else:\n",
    "            dk=-np.linalg.inv(Hessian(function,xk))*gk\n",
    "        alpha=Armijo(function,xk,dk,0.5)\n",
    "        xk=xk+alpha*dk\n",
    "        gk=grad(function,xk)\n",
    "    return xk\n",
    "\n",
    "def Graddecent(function,x,epsilon):# 最速梯度法\n",
    "    xk=x\n",
    "    gk=grad(function,xk)\n",
    "    max_iter=10000\n",
    "    cnt=0\n",
    "    while(np.linalg.norm(gk)>epsilon):\n",
    "        dk=-grad(function,xk)\n",
    "        alpha=Armijo(function,xk,dk,0.5)\n",
    "        xk=xk+dk*alpha\n",
    "        gk=grad(function,xk)\n",
    "        cnt+=1\n",
    "        if cnt==max_iter:\n",
    "            break\n",
    "    return xk\n",
    "\n",
    "def penaltyfunctionmethod(x0,miu0,epsilon):\n",
    "    start=time.time()\n",
    "    cnt=0\n",
    "    xk=x0\n",
    "    miuk=miu0\n",
    "    while True:\n",
    "        xk=dampnewton(lambda x:penaltyfunction(x,miuk),xk,epsilon)\n",
    "        if equalization(xk)+inequalization(xk)<epsilon:\n",
    "            break\n",
    "        miuk=miuk*10\n",
    "        cnt+=1\n",
    "        print(\"cnt=\",cnt)\n",
    "    end=time.time()-start\n",
    "    return xk,end,cnt\n",
    "\n",
    "X=np.mat([2.0,2.0,2.0]).T\n",
    "Xtrue=np.mat([1.104859024,1.196674194,1.535262257]).T\n",
    "miu=1\n",
    "epsilon=1e-6\n",
    "xk,end,cnt=penaltyfunctionmethod(X,miu,epsilon)\n",
    "print(\"最优解为：\",xk.T)\n",
    "print(\"最优值为：\",targetfunction(xk))\n",
    "print(\"迭代次数为：\",cnt)\n",
    "print(\"运行时间为：\",end)\n",
    "print(\"理想最优解：\",targetfunction(Xtrue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e40a2a0f1c3030",
   "metadata": {},
   "source": [
    "可知，外点罚函数法运行时间较长，可能与采用armijo非精确线搜索方法相关，同时也由于此方法有数值困难。导致使用最速梯度法运行时间能够达到370s，过于低效率，使用阻尼牛顿法有明显改善。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e346ee3599718a",
   "metadata": {},
   "source": [
    "# 拉格朗日乘子法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ab8c8ebe26f3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T07:56:19.231001Z",
     "start_time": "2024-05-15T07:56:18.556743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt= 1\n",
      "cnt= 2\n",
      "cnt= 3\n",
      "cnt= 4\n",
      "cnt= 5\n",
      "cnt= 6\n",
      "cnt= 7\n",
      "cnt= 8\n",
      "cnt= 9\n",
      "cnt= 10\n",
      "cnt= 11\n",
      "cnt= 12\n",
      "cnt= 13\n",
      "cnt= 14\n",
      "最优解为： [[1.10488916 1.19670324 1.53525192]]\n",
      "最优值为： [[0.03256821]]\n",
      "迭代次数为： 14\n",
      "运行时间为： 0.63747239112854\n",
      "理想最优解： [[0.0325682]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "def targetfunction(x):# x是列向量\n",
    "    return (x[0]-1)**2+(x[0]-x[1])**2+(x[1]-x[2])**4\n",
    "\n",
    "def limitation(x):# 约束条件，x是列向量\n",
    "    if -10 <= x[0] <= 10 and -10 <= x[1] <= 10 and -10 <= x[2] <= 10:\n",
    "        if x[0]*(1+x[1]**2)+x[2]**3-4-3*math.sqrt(2)==0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def equalization(x):# 等式约束条件，x是列向量\n",
    "    return x[0]*(1+x[1]**2)+x[2]**4-4-3*math.sqrt(2)\n",
    "\n",
    "def inequalization(x):# 不等式约束条件，x是列向量\n",
    "    return (min(x[0]+10,0))**2+(min(10-x[0],0))**2+(min(x[1]+10,0))**2+(min(10-x[1],0))**2+(min(x[2]+10,0))**2+(min(10-x[2],0))**2\n",
    "\n",
    "def ita(lamda,miu):\n",
    "    return lamda/miu\n",
    "\n",
    "def fi(ci,x,lamda,miu):\n",
    "    if ci(x)-ita(lamda,miu)>=0:\n",
    "        return -0.5*miu*ita(lamda,miu)**2\n",
    "    else:\n",
    "        return 0.5*miu*((ci(x)-ita(lamda,miu))**2-ita(lamda,miu)**2)\n",
    "    \n",
    "def interfi(x,lamda,miu):# 所有不等式约束条件转化成的fi\n",
    "    limit1=lambda x:10-x\n",
    "    limit2=lambda x:x+10\n",
    "    return fi(limit1,x[0],lamda[0],miu)+fi(limit2,x[0],lamda[1],miu)+fi(limit1,x[1],lamda[2],miu)+fi(limit2,x[1],lamda[3],miu)+fi(limit1,x[2],lamda[4],miu)+fi(limit2,x[2],lamda[5],miu)\n",
    "\n",
    "def lagrangefunction(x,lamda,miu):# 拉格朗日函数\n",
    "    return targetfunction(x)-lamda[0]*equalization(x)+0.5*miu*(equalization(x)**2)+interfi(x,lamda[1:],miu)\n",
    "\n",
    "def endtest(x,lamda,miu,epsilon):# 终止条件\n",
    "    limit1=lambda x:10-x\n",
    "    limit2=lambda x:x+10\n",
    "    return math.sqrt(targetfunction(x)**2+min(ita(lamda[1],miu),limit1(x[0]))**2+min(ita(lamda[2],miu),limit2(x[0]))**2+min(ita(lamda[3],miu),limit1(x[1]))**2+min(ita(lamda[4],miu),limit2(x[1]))**2+min(ita(lamda[5],miu),limit1(x[2]))**2+min(ita(lamda[6],miu),limit2(x[2]))**2)<=epsilon\n",
    "\n",
    "def endtest2(x,epsilon):# 改进后的终止条件\n",
    "    limit1=lambda x:10-x\n",
    "    limit2=lambda x:x+10\n",
    "    return np.linalg.norm(equalization(x))+np.linalg.norm(np.mat([min(limit1(x[0]),0),min(limit2(x[0]),0),min(limit1(x[1]),0),min(limit2(x[1]),0),min(limit1(x[2]),0),min(limit2(x[2]),0)]).T) <=epsilon\n",
    "\n",
    "def lamdaiter(x,lamda,miu):\n",
    "    limit1=lambda x:10-x\n",
    "    limit2=lambda x:x+10\n",
    "    lamda[0]=lamda[0]-miu*equalization(x)\n",
    "    lamda[1]=-miu*min(limit1(x[0])-ita(lamda[1],miu),0)\n",
    "    lamda[2]=-miu*min(limit2(x[0])-ita(lamda[2],miu),0)\n",
    "    lamda[3]=-miu*min(limit1(x[1])-ita(lamda[3],miu),0)\n",
    "    lamda[4]=-miu*min(limit2(x[1])-ita(lamda[4],miu),0)\n",
    "    lamda[5]=-miu*min(limit1(x[2])-ita(lamda[5],miu),0)\n",
    "    lamda[6]=-miu*min(limit2(x[2])-ita(lamda[6],miu),0)\n",
    "    return lamda\n",
    "\n",
    "def grad(f, x):\n",
    "    '''\n",
    "    求梯度\n",
    "    :param f: 函数\n",
    "    :param x: 向量\n",
    "    :return: 梯度向量\n",
    "    '''\n",
    "    delta=0.0000001\n",
    "    gradmatrix=np.zeros(x.shape)\n",
    "    fx=f(x)\n",
    "    it=np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix=it.multi_index\n",
    "        old_value=(np.float64)(x[ix])\n",
    "        x[ix]=(np.float64)(old_value+delta)\n",
    "        fxd=f(x)\n",
    "        gradmatrix[ix]=(fxd-fx)/delta\n",
    "        x[ix]=old_value\n",
    "        it.iternext()\n",
    "    return np.mat(gradmatrix)\n",
    "\n",
    "def Hessian(f,x):\n",
    "    '''\n",
    "    求Hessian矩阵\n",
    "    :param f: 函数\n",
    "    :param x: 初始点\n",
    "    :param epsilon: epsilon\n",
    "    :return: f在x处的Hessian矩阵\n",
    "    '''\n",
    "    delta=1e-3\n",
    "    n=np.size(x)\n",
    "    HessianMatrix=np.zeros((n,n))\n",
    "    gx0=grad(f,x)\n",
    "    for i in range(0,n):\n",
    "        old_value = (np.float64)(x[i,0])\n",
    "        x[i,0] = (np.float64)(old_value + delta)\n",
    "        gxk=grad(f,x)\n",
    "        for j in range(0,n):\n",
    "            HessianMatrix[i,j]=(np.float64)((gxk[j,0]-gx0[j,0])/delta)\n",
    "        x[i,0]=old_value\n",
    "    return np.mat(HessianMatrix)\n",
    "\n",
    "def Armijo(f,xk,dk,rho):# 非精确线搜索，采用armijo方法\n",
    "    alpha=1\n",
    "    beta=rho\n",
    "    sigma=0.5\n",
    "    max_iter=5000\n",
    "    cnt=0\n",
    "    while True:\n",
    "        if f(xk+alpha*dk)<=f(xk)+sigma*alpha*grad(f,xk).T*dk:\n",
    "            break\n",
    "        alpha*=beta\n",
    "        cnt+=1\n",
    "        if cnt==max_iter:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "def dampnewton(function,x,epsilon):\n",
    "    xk=x\n",
    "    gk=grad(function,xk)\n",
    "    max_iter=10000\n",
    "    cnt=0\n",
    "    while(np.linalg.norm(gk)>epsilon):\n",
    "        if (np.linalg.det(Hessian(function,xk))<epsilon):\n",
    "            dk=-gk\n",
    "        else:\n",
    "            dk=-np.linalg.inv(Hessian(function,xk))*gk\n",
    "        alpha=Armijo(function,xk,dk,0.5)\n",
    "        xk=xk+alpha*dk\n",
    "        gk=grad(function,xk)\n",
    "        cnt+=1\n",
    "        if abs(function(xk)-function(xk-alpha*dk))<=epsilon:\n",
    "            break\n",
    "        if cnt==max_iter:\n",
    "            break\n",
    "    return xk\n",
    "\n",
    "def Graddecent(function,x,epsilon):\n",
    "    xk=x\n",
    "    gk=grad(function,xk)\n",
    "    max_iter=10000\n",
    "    cnt=0\n",
    "    while(np.linalg.norm(gk)>epsilon):\n",
    "        dk=-grad(function,xk)\n",
    "        alpha=Armijo(function,xk,dk,0.5)\n",
    "        xk=xk+dk*alpha\n",
    "        gk=grad(function,xk)\n",
    "        cnt+=1\n",
    "        if cnt==max_iter:\n",
    "            break\n",
    "    return xk\n",
    "\n",
    "def lagrangemethod(x0,lamda0,miu0,epsilon,rou):\n",
    "    start=time.time()\n",
    "    max_iter=100\n",
    "    cnt=0\n",
    "    xk=x0\n",
    "    lamdak=lamda0\n",
    "    miuk=miu0\n",
    "    while True:\n",
    "        xlast=xk\n",
    "        xk=dampnewton(lambda x:lagrangefunction(x,lamdak,miuk),xk,epsilon)\n",
    "        lamdak=lamdaiter(xk,lamdak,miuk)\n",
    "        cnt+=1\n",
    "        print(\"cnt=\",cnt)\n",
    "        # if endtest(xk,lamdak,miuk,epsilon):\n",
    "        if endtest2(xk,epsilon):\n",
    "            break\n",
    "        miuk=miuk*rou\n",
    "        if cnt==max_iter:\n",
    "            break\n",
    "    end=time.time()-start\n",
    "    return xk,end,cnt    \n",
    "\n",
    "\n",
    "X=np.mat([2.0,2.0,2.0]).T\n",
    "Xtrue=np.mat([1.104859024,1.196674194,1.535262257]).T\n",
    "lamda=np.mat([1.0,1.0,1.0,1.0,1.0,1.0,1.0]).T\n",
    "miu=1\n",
    "epsilon=1e-6\n",
    "rou=2\n",
    "xk,end,cnt=lagrangemethod(X,lamda,miu,epsilon,rou)\n",
    "print(\"最优解为：\",xk.T)\n",
    "print(\"最优值为：\",targetfunction(xk))\n",
    "print(\"迭代次数为：\",cnt)\n",
    "print(\"运行时间为：\",end)\n",
    "print(\"理想最优解：\",targetfunction(Xtrue))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9fccf687d0cfd",
   "metadata": {},
   "source": [
    "在低精度1e-4情况下，拉格朗日乘子法有较多的迭代次数，但是运行时间得到了提升。\n",
    "\n",
    "在高精度1e-6情况下，拉格朗日乘子法在运行时间和迭代次数上均大于外点罚函数法，推测由于终止条件没有较好改进所致。\n",
    "\n",
    "两种方法都能得到与最优解极其接近的结果，效果较好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
